{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp78_Reinforcement_learning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dLWZAr0gD_Xz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Basic reinforcement learning example, openAI Gym environment\n",
        "\n",
        "## Goal : \n",
        "The goal is to be familiarized with the #openAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "3vYweJ9MK9s-",
        "colab_type": "code",
        "outputId": "368ca82a-f4c4-4bd2-9c7e-e554adabd90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3YqKpYDEDUDz",
        "colab_type": "code",
        "outputId": "8f029335-c8ae-4d24-c205-8d07463efbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Creating thr env\n",
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "env.s = 328\n",
        "\n",
        "\n",
        "# Setting the number of iterations, penalties and reward to zero,\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    # Put each rendered frame into the dictionary for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "    }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))\n",
        "\n",
        "# Printing all the possible actions, states, rewards.\n",
        "def renderFrames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue() )\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "renderFrames(frames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 256\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AVGyUydTMhJR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q-learning solution"
      ]
    },
    {
      "metadata": {
        "id": "bQAW2_euMfoM",
        "colab_type": "code",
        "outputId": "0f5eca68-ad4f-4145-dbe4-15a9f1f04a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Init Taxi-V2 Env\n",
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "# Init arbitary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1    # learning rate\n",
        "gamma = 0.6    #initial 0.6 discount factor\n",
        "epsilon = 0.1\n",
        "\n",
        "\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "all_rewards = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Init Vars\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Check the action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Check the learned values\n",
        "            action = np.argmax(q_table[state])  # Utilisation des valeurs déjà apprise\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        print()\n",
        "        \n",
        "        all_rewards.append(reward) # garde les rewards\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        # Update the new value\n",
        "        new_value = (1 - alpha) * old_value + alpha * \\\n",
        "            (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\" Episode: { i }\")\n",
        "print(\"Penalties \", all_penalties)\n",
        "print(\"Rewards \", all_penalties)\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ pour le rapport\n",
        "# SARSA : https://blog.goodaudience.com/attempting-open-ais-taxi-v2-using-the-sarsa-max-algorithm-70a4de8c8c9c "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Episode: 100000\n",
            "Penalties  []\n",
            "Rewards  []\n",
            "Training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q-b2DdC7ZTMn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Plus gamma(discount factor ) est élévé il va cherché à considerer un gros grand reward. Learning rate (alpha)"
      ]
    },
    {
      "metadata": {
        "id": "-_oDIIYWVx7r",
        "colab_type": "code",
        "outputId": "2c89ccd9-3b87-4b2a-a055-516c222b485c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "print(q_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -2.27325181  -2.12208639  -2.27325183  -2.12208639  -1.870144\n",
            "  -11.12208579]\n",
            " [ -1.87014345  -1.45024005  -1.87014398  -1.45024006  -0.7504\n",
            "  -10.45023967]\n",
            " ...\n",
            " [ -0.95844332   0.416       -0.91357461  -1.19075874  -6.39694391\n",
            "   -6.07935937]\n",
            " [ -2.15942823  -2.12204103  -2.14648392  -2.12203985  -7.17923381\n",
            "   -7.15523852]\n",
            " [  2.17751996   1.3697416    3.76474205  11.          -2.60602997\n",
            "   -2.75102621]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fTtNimqRatD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using the trained Q_table to test "
      ]
    },
    {
      "metadata": {
        "id": "9hITA48nRkXY",
        "colab_type": "code",
        "outputId": "8c3f416e-a73c-443c-ec9d-c95063b4e83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "test_episodes = 100000\n",
        "env.reset()    # réinitialisation de l'environnement\n",
        "rewards =[]   \n",
        "\n",
        "for e in range(test_episodes):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done :\n",
        "        #env.render()\n",
        "        action = np.argmax(q_table[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        state = new_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            rewards.append(total_reward)\n",
        "            break\n",
        "env.close()    \n",
        "print('the score is: {}'.format(sum(rewards)/test_episodes))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the score is: 8.46569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KYBQFtw0dtdV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SARSA learning algorithm"
      ]
    },
    {
      "metadata": {
        "id": "vi_29bQCbuWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "env.s = 328\n",
        "\n",
        "# Init arbitary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "#hyperparameters\n",
        "alpha = 0.85\n",
        "gamma = 0.90\n",
        "epsilon = 0.8\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0v39q68eA9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rewards=[]\n",
        "\n",
        "for i in range(30000):\n",
        "  # we store cumulative reward of each episodes in r\n",
        "  r = 0\n",
        "  # initialize the state,\n",
        "  state = env.reset()\n",
        "  # select the action \n",
        "  if random.uniform(0,1) < epsilon :\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = np.argmax(q_table[state])\n",
        "      \n",
        "  done = False\n",
        "  while not done:\n",
        "       \n",
        "    # then we perform the action and move to the next state, and receive the reward\n",
        "    nextstate, reward, done, info = env.step(action)\n",
        "        \n",
        "    # again, we select the next action \n",
        "    if random.uniform(0,1) < epsilon :\n",
        "      nextaction = env.action_space.sample()\n",
        "    else:\n",
        "      nextaction = np.argmax(q_table[state])\n",
        "    \n",
        "    # we calculate the Q value of previous state using our update rule\n",
        "    q_table[(state,action)] += alpha * (reward + gamma * q_table[(nextstate,nextaction)]-q_table[(state,action)])\n",
        "\n",
        "    # finally we update our state and action with next action and next state\n",
        "    action = nextaction\n",
        "    state = nextstate\n",
        "        \n",
        "    # store the rewards\n",
        "    r+=reward\n",
        "     \n",
        "    if done:\n",
        "      rewards.append(r)\n",
        "  #print(rewards[i])\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "urbUYtWJzD6y",
        "colab_type": "code",
        "outputId": "b07ea7bb-62be-44d9-f6c6-b40e70508a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "print(q_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [-33.16102325 -28.42239026 -29.59155876 -23.72550622 -31.40559313\n",
            "  -33.96014659]\n",
            " [-33.44367501 -29.82959485 -29.42912475 -26.06650958 -26.23155519\n",
            "  -31.26726039]\n",
            " ...\n",
            " [-22.91617867 -30.88381015 -25.61548424 -28.19666113 -30.6474348\n",
            "  -32.26941425]\n",
            " [-22.12638081 -33.84549716 -29.5881619  -33.5094775  -31.90831879\n",
            "  -35.62921668]\n",
            " [-21.36829045 -22.16514535 -28.95468587 -11.62899783  -6.97042712\n",
            "  -23.00722792]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vaybKISYsogq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_episodes = 10000\n",
        "env.reset()    # réinitialisation de l'environnement\n",
        "rewards =[]   \n",
        "\n",
        "done = False\n",
        "\n",
        "for e in range(test_episodes):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done :\n",
        "        #env.render()\n",
        "        action = np.argmax(q_table[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        state = new_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            rewards.append(total_reward)\n",
        "            break\n",
        "env.close()    \n",
        "print('the score is: {}'.format(sum(rewards)/test_episodes))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}