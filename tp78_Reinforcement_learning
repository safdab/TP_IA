{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tp78_Reinforcement_learning","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"dLWZAr0gD_Xz","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","# Basic reinforcement learning example, openAI Gym environment\n","\n","## Goal : \n","The goal is to be familiarized with the #openAI Gym"]},{"metadata":{"id":"3vYweJ9MK9s-","colab_type":"code","outputId":"f789c305-e2a4-4ee0-fb64-ede56bc6c7ca","executionInfo":{"status":"ok","timestamp":1553002668947,"user_tz":-60,"elapsed":5532,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"cell_type":"code","source":["!pip install gym"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"metadata":{"id":"3YqKpYDEDUDz","colab_type":"code","outputId":"36da12a9-9270-4928-9d82-a92239a1a56f","executionInfo":{"status":"ok","timestamp":1553014566735,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"cell_type":"code","source":["import gym\n","from time import sleep\n","from IPython.display import clear_output\n","\n","# Creating thr env\n","env = gym.make(\"Taxi-v2\").env\n","\n","env.s = 328\n","\n","\n","# Setting the number of iterations, penalties and reward to zero,\n","epochs = 0\n","penalties, reward = 0, 0\n","\n","frames = []\n","\n","done = False\n","\n","while not done:\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","\n","    # Put each rendered frame into the dictionary for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","    }\n","    )\n","\n","    epochs += 1\n","\n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))\n","\n","# Printing all the possible actions, states, rewards.\n","def renderFrames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'].getvalue() )\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.1)\n","        \n","renderFrames(frames)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |B: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 184\n","State: 0\n","Action: 5\n","Reward: 20\n"],"name":"stdout"}]},{"metadata":{"id":"AVGyUydTMhJR","colab_type":"text"},"cell_type":"markdown","source":["# Q-learning solution"]},{"metadata":{"id":"bQAW2_euMfoM","colab_type":"code","outputId":"f63e5f83-4e36-4537-fac6-a801c3a9865b","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1553014934323,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","from IPython.display import clear_output\n","\n","# Init Taxi-V2 Env\n","env = gym.make(\"Taxi-v2\").env\n","\n","# Init arbitary values\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Hyperparameters\n","alpha = 0.1    # learning rate\n","gamma = 0.6    #initial 0.6 discount factor\n","epsilon = 0.1\n","\n","\n","all_epochs = []\n","all_penalties = []\n","all_rewards = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    # Init Vars\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            # Check the action space\n","            action = env.action_space.sample()\n","        else:\n","            # Check the learned values\n","            action = np.argmax(q_table[state])  # Utilisation des valeurs déjà apprise\n","\n","        next_state, reward, done, info = env.step(action)\n","        print()\n","        \n","        all_rewards.append(reward) # garde les rewards\n","\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        # Update the new value\n","        new_value = (1 - alpha) * old_value + alpha * \\\n","            (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\" Episode: { i }\")\n","print(\"Penalties \", all_penalties)\n","print(\"Rewards \", all_penalties)\n","\n","print(\"Training finished.\")\n","\n","# https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ pour le rapport\n","# SARSA : https://blog.goodaudience.com/attempting-open-ais-taxi-v2-using-the-sarsa-max-algorithm-70a4de8c8c9c "],"execution_count":49,"outputs":[{"output_type":"stream","text":[" Episode: 100000\n","Penalties  []\n","Rewards  []\n","Training finished.\n"],"name":"stdout"}]},{"metadata":{"id":"q-b2DdC7ZTMn","colab_type":"text"},"cell_type":"markdown","source":["# Conclusion\n","Plus gamma(discount factor ) est élévé il va cherché à considerer un gros grand reward. Learning rate (alpha"]},{"metadata":{"id":"-_oDIIYWVx7r","colab_type":"code","outputId":"9cd715d2-0e4f-444e-bda0-cdf7e14e288a","executionInfo":{"status":"ok","timestamp":1553005491926,"user_tz":-60,"elapsed":701,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"cell_type":"code","source":["print(q_table)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[  0.           0.           0.           0.           0.\n","    0.        ]\n"," [ -2.27325175  -2.12208639  -2.27325183  -2.12208639  -1.870144\n","  -11.12208591]\n"," [ -1.87014397  -1.45024     -1.87014397  -1.45024001  -0.7504\n","  -10.45023933]\n"," ...\n"," [ -1.03508692   0.416       -1.05630266  -1.3752901   -5.64893591\n","   -6.52276468]\n"," [ -2.15882367  -2.1220592   -2.17918327  -2.12205737  -7.07936516\n","   -5.69963548]\n"," [  2.23169146   1.29490707   3.14633664  11.          -2.8834911\n","   -1.97765133]]\n"],"name":"stdout"}]},{"metadata":{"id":"5fTtNimqRatD","colab_type":"text"},"cell_type":"markdown","source":["# Using the trained Q_table to test "]},{"metadata":{"id":"9hITA48nRkXY","colab_type":"code","outputId":"de813e97-8d79-430b-eaa8-c58b629a1f5d","executionInfo":{"status":"ok","timestamp":1553014962740,"user_tz":-60,"elapsed":16034,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["\n","test_episodes = 100000\n","env.reset()    # réinitialisation de l'environnement\n","rewards =[]   \n","\n","for e in range(test_episodes):\n","    done = False\n","    state = env.reset()\n","    total_reward = 0\n","    while not done :\n","        #env.render()\n","        action = np.argmax(q_table[state,:])\n","        new_state, reward, done, info = env.step(action)\n","        state = new_state\n","        total_reward += reward\n","        if done:\n","            rewards.append(total_reward)\n","            break\n","env.close()    \n","print('the score is: {}'.format(sum(rewards)/test_episodes))\n","\n"],"execution_count":50,"outputs":[{"output_type":"stream","text":["the score is: 8.47209\n"],"name":"stdout"}]},{"metadata":{"id":"KYBQFtw0dtdV","colab_type":"text"},"cell_type":"markdown","source":["# SARSA learning algorithm"]},{"metadata":{"id":"vi_29bQCbuWR","colab_type":"code","colab":{}},"cell_type":"code","source":["env = gym.make(\"Taxi-v2\").env\n","\n","env.s = 328\n","\n","# Init arbitary values\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","#hyperparameters\n","alpha = 0.85\n","gamma = 0.90\n","epsilon = 0.8\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F0v39q68eA9O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":600},"outputId":"8f9d970e-2e98-46f5-e116-81a60eee9584","executionInfo":{"status":"error","timestamp":1553015452686,"user_tz":-60,"elapsed":467769,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}}},"cell_type":"code","source":["rewards=[]\n","\n","for i in range(30000):\n","  # we store cumulative reward of each episodes in r\n","  r = 0\n","  # initialize the state,\n","  state = env.reset()\n","  # select the action \n","  if random.uniform(0,1) < epsilon :\n","    action = env.action_space.sample()\n","  else:\n","    action = np.argmax(q_table[state])\n","      \n","  done = False\n","  while not done:\n","       \n","    # then we perform the action and move to the next state, and receive the reward\n","    nextstate, reward, done, _ = env.step(action)\n","        \n","    # again, we select the next action \n","    if random.uniform(0,1) < epsilon :\n","      nextaction = env.action_space.sample()\n","    else:\n","      nextaction = np.argmax(q_table[state])\n","    \n","    # we calculate the Q value of previous state using our update rule\n","    q_table[(state,action)] += alpha * (reward + gamma * q_table[(nextstate,nextaction)]-q_table[(state,action)])\n","\n","    # finally we update our state and action with next action and next state\n","    action = nextaction\n","    state = nextstate\n","        \n","    # store the rewards\n","    r+=reward\n","     \n","    if done:\n","      rewards.append(r)\n","  #print(rewards[i])\n","        \n","env.close()"],"execution_count":52,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-335d3ed48450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# then we perform the action and move to the next state, and receive the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnextstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# again, we select the next action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mEach\u001b[0m \u001b[0mrow\u001b[0m \u001b[0mspecifies\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"urbUYtWJzD6y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"fa34579d-d61e-4e19-e10e-7dff690b8507","executionInfo":{"status":"ok","timestamp":1553013973228,"user_tz":-60,"elapsed":1415,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}}},"cell_type":"code","source":["print(q_table)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[[  0.           0.           0.           0.           0.\n","    0.        ]\n"," [-26.8017037  -33.36046749 -30.98938351 -25.62332424 -23.5099583\n","  -31.07116251]\n"," [-31.73043873 -25.80968995 -28.30490103 -32.66499653 -27.90762112\n","  -32.30552754]\n"," ...\n"," [-23.70412703 -25.99241069 -28.27166244 -29.37604006 -24.59408723\n","  -31.75832141]\n"," [-23.97824986 -31.31346314 -30.87476729 -31.4584631  -36.90368792\n","  -40.46500229]\n"," [  0.62707643 -25.51456813 -13.8907031  -15.40425058 -16.12357161\n","  -25.86902967]]\n"],"name":"stdout"}]},{"metadata":{"id":"vaybKISYsogq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":600},"outputId":"67b11ccf-2384-4cef-c6e4-36253bf68921","executionInfo":{"status":"error","timestamp":1553014524123,"user_tz":-60,"elapsed":2979,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}}},"cell_type":"code","source":["test_episodes = 1000\n","env.reset()    # réinitialisation de l'environnement\n","rewards =[]   \n","\n","done = False\n","\n","for e in range(test_episodes):\n","    done = False\n","    state = env.reset()\n","    total_reward = 0\n","    while not done :\n","        #env.render()\n","        action = np.argmax(q_table[state,:])\n","        new_state, reward, done, info = env.step(action)\n","        state = new_state\n","        total_reward += reward\n","        if done:\n","            rewards.append(total_reward)\n","            break\n","env.close()    \n","print('the score is: {}'.format(sum(rewards)/test_episodes))"],"execution_count":47,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-37944557be46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mEach\u001b[0m \u001b[0mrow\u001b[0m \u001b[0mspecifies\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}