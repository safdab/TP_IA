{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp3_4_Text_Classification_Ohsumed.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DhDJAoZvPfCV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xvzf ohsumed-first-20000-docs.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhPwVqA7STsV",
        "colab_type": "code",
        "outputId": "aeab7b88-1dab-461a-b2a8-942dca08c55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import math\n",
        "import nltk\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9fmaHFUCgtf4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qJpm9epPVXjG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "pathTrainingDir = 'ohsumed-first-20000-docs/training/'\n",
        "pathTestDir = 'ohsumed-first-20000-docs/test/'\n",
        "\n",
        "#stop_words = set(stopwords.words('english')).union(set(list(punctuation)))\n",
        "\n",
        "listCategories = os.listdir(pathTrainingDir)\n",
        "# Dictionnaire qui associe chaque categorie aux différents articles qui font référence à cette dernière\n",
        "dictCatgoriesArticles_training = {}\n",
        "dictCategoriesArticles_test = {}\n",
        "\n",
        "for categorie in listCategories :\n",
        "  Newpath = pathTrainingDir+categorie+'/'\n",
        "  NewpathTest = pathTestDir+categorie+'/'\n",
        "  # Liste des fichiers contenus dans une categorie\n",
        "  articlesTraining = os.listdir(Newpath)\n",
        "  articlestest = os.listdir(NewpathTest)\n",
        "  \n",
        "  #categorie = categorie.replace('C','')\n",
        "  dictCatgoriesArticles_training[categorie] = articlesTraining\n",
        "  dictCategoriesArticles_test[categorie] = articlestest;\n",
        "\n",
        "\n",
        "   \n",
        "print(dictCatgoriesArticles_training.items())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Koi9X8rGzWF3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#   Liaison entre les articles et leur contenu"
      ]
    },
    {
      "metadata": {
        "id": "1oOtWeOKgh0C",
        "colab_type": "code",
        "outputId": "be5845c0-ff38-45f8-be70-8d37985a15be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "# Pour le training part\n",
        "dictTraining_ArticlesText = {}\n",
        "for key in dictCatgoriesArticles_training.keys() : \n",
        "  allArticles = dictCatgoriesArticles_training[key] # on recupère tous les articles de la categorie\n",
        "  for article in allArticles :\n",
        "    FilePath = pathTrainingDir + key+'/'+article\n",
        "    fileContent = \"\"\n",
        "    with io.open(FilePath, \"r\", newline=None) as fd:\n",
        "      for line in fd:\n",
        "        line = line.replace(\"\\n\", \"\")\n",
        "        fileContent = fileContent + line\n",
        "    \n",
        "    dictTraining_ArticlesText[article] = fileContent;    \n",
        "\n",
        "\n",
        "  \n",
        "# Pour le test part\n",
        "dictTest_ArticlesText = {}\n",
        "\n",
        "for key in dictCategoriesArticles_test.keys() : \n",
        "  allArticles = dictCategoriesArticles_test[key] # on recupère tous les articles de la categorie\n",
        "  for article in allArticles :\n",
        "    FilePath = pathTestDir + key +'/'+ article\n",
        "    #file = open(FilePath, 'r') \n",
        "    fileContent = \"\"\n",
        "    with io.open(FilePath, \"r\", newline=None) as fd:\n",
        "      for line in fd:\n",
        "        line = line.replace(\"\\n\", \"\")\n",
        "        fileContent = fileContent + line\n",
        "        \n",
        "    dictTest_ArticlesText[article] = fileContent;    \n",
        "  #  print(fileContent)\n",
        " #   break\n",
        " # break\n",
        "print(dictTest_ArticlesText)\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kyv4OozFE0Xy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Liaison de chaque article aux différentes catégories auxquelles il appartient\n",
        "Sachant qu'un même article peut se retrouver dans différentes catégories donc on fait un dictionnaire dont la clé est le nom de l'article et la valeur une liste des catégories auxquelles ce dernier appartient. On élimine en même temps le caractère C dans le nom de la catégorie afin de garder que des entiers. par exemple, la catégorie C1 sera representée par 1.."
      ]
    },
    {
      "metadata": {
        "id": "hqiB8ZjBEzIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pour les articles du training data\n",
        "dict_article_categories_training = {}\n",
        "\n",
        "for cat, articles in dictCatgoriesArticles_training.items():\n",
        "  for article in articles : \n",
        "    if article in dict_article_categories_training.keys():\n",
        "      dict_article_categories_training[article].append(int(cat.replace(\"C\", \"\")))\n",
        "    else :\n",
        "      dict_article_categories_training[article] = [int(cat.replace(\"C\", \"\"))]\n",
        "        \n",
        "print(dict_article_categories_training)\n",
        "  \n",
        "# Pour les articles du test data\n",
        "dict_article_categories_test = {}\n",
        "\n",
        "for cat, articles in dictCategoriesArticles_test.items():\n",
        "  for article in articles : \n",
        "    if article in dict_article_categories_test.keys():\n",
        "      dict_article_categories_test[article].append(int(cat.replace(\"C\", \"\")))\n",
        "    else :\n",
        "      dict_article_categories_test[article] = [int(cat.replace(\"C\", \"\"))]\n",
        "        \n",
        "print(dict_article_categories_test)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0sdlOH-RbwkW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Labelisation des différentes catégories \n",
        "On choisit d'encapsuler chaque article dans un vecteur de taille 23 (correspondant au nombre de catégories) et on met à 1 à chaque fois que l'article appartient à une catégorie donnée."
      ]
    },
    {
      "metadata": {
        "id": "ZFEoKJ2v3_V6",
        "colab_type": "code",
        "outputId": "2aa0297a-76e4-44bd-83e6-cba8ca5027a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# initilisation d'un vecteur de taille 23 à zero\n",
        "vecteur = []\n",
        "for i in range(23):\n",
        "  vecteur.append(0)\n",
        "  \n",
        "# Partie training\n",
        "\n",
        "y_train = []\n",
        "\n",
        "\n",
        "for listCategorie in dict_article_categories_training.keys():\n",
        "  vect_copy = vecteur.copy()\n",
        "  labels = dict_article_categories_training[listCategorie] #labels \n",
        "  for label in labels :\n",
        "    vect_copy[label-1] = 1\n",
        "  y_train.append(vect_copy)\n",
        "  \n",
        "\n",
        "# Partie test\n",
        "\n",
        "\n",
        "y_test = []\n",
        "\n",
        "\n",
        "for listCategorie in dict_article_categories_test.keys():\n",
        "  vect_copy = vecteur.copy()\n",
        "  labels = dict_article_categories_test[listCategorie] #labels \n",
        "  for label in labels :\n",
        "    vect_copy[label-1] = 1\n",
        "  y_test.append(vect_copy)\n",
        "  \n",
        "# Transformation des listes de labels en des numpy array\n",
        "y_train = np.asarray(y_train)\n",
        "y_test  = np.asarray(y_test)\n",
        "    \n",
        "print(len(vecteur))\n",
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aoo9WYEFEfgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "o6GtbEZBb_IT",
        "colab_type": "code",
        "outputId": "a04208d4-1333-4aba-ddf3-a2d7e75ae86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# Recuperation de tous les contenus des articles pour le train et pour le test\n",
        "\n",
        "\n",
        "x_train_text = list()\n",
        "x_train_text_temp = list()\n",
        "\n",
        "x_test_text = list()\n",
        "x_test_text_temp = list()\n",
        "\n",
        "for key,text in dictTest_ArticlesText.items():\n",
        "  x_test_text_temp.append(text)\n",
        "  \n",
        "for key,text in dictTraining_ArticlesText.items():\n",
        "  x_train_text_temp.append(text)\n",
        "  \n",
        "t = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "#t.fit_on_texts(x_test_text_temp + x_train_text_temp)\n",
        "    \n",
        "x_test_text = t.texts_to_sequences(x_test_text_temp)\n",
        "x_train_text = t.texts_to_sequences(x_train_text_temp)\n",
        "  \n",
        "x_train_text = sequence.pad_sequences(x_train_text, maxlen=420)\n",
        "x_test_text = sequence.pad_sequences(x_test_text, maxlen=420)\n",
        "  \n",
        "print('x_train_text shape:', x_train_text.shape)\n",
        "print('x_test_text shape:', x_test_text.shape)\n",
        "\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_text shape: (6286, 420)\n",
            "x_test_text shape: (7643, 420)\n",
            "y_train shape: (6286, 23)\n",
            "y_test shape: (7643, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "on4Q7JlAVNCc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the model\n"
      ]
    },
    {
      "metadata": {
        "id": "CGwXvi6DorRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "28dacaff-75da-4567-93c2-7952f5044e21"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "max_features = 300000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 24\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(48, dropout=0.5, recurrent_dropout=0.3))\n",
        "model.add(Dense(23, activation='softmax'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train_text, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test_text, y_test))\n",
        "score, acc = model.evaluate(x_test_text, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 6286 samples, validate on 7643 samples\n",
            "Epoch 1/5\n",
            "6286/6286 [==============================] - 131s 21ms/step - loss: 0.2442 - acc: 0.9278 - val_loss: 0.2438 - val_acc: 0.9276\n",
            "Epoch 2/5\n",
            "6286/6286 [==============================] - 135s 21ms/step - loss: 0.2415 - acc: 0.9278 - val_loss: 0.2435 - val_acc: 0.9276\n",
            "Epoch 3/5\n",
            "6286/6286 [==============================] - 134s 21ms/step - loss: 0.2415 - acc: 0.9278 - val_loss: 0.2433 - val_acc: 0.9276\n",
            "Epoch 4/5\n",
            "6286/6286 [==============================] - 134s 21ms/step - loss: 0.2415 - acc: 0.9278 - val_loss: 0.2436 - val_acc: 0.9276\n",
            "Epoch 5/5\n",
            "6286/6286 [==============================] - 133s 21ms/step - loss: 0.2415 - acc: 0.9278 - val_loss: 0.2436 - val_acc: 0.9276\n",
            "7643/7643 [==============================] - 15s 2ms/step\n",
            "Test score: 0.24358691790996695\n",
            "Test accuracy: 0.9275665795726644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z9D0eIr0zI_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Après la construction du model on constate qu'un obtient un accurracy de 92% ."
      ]
    }
  ]
}