{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tp1_2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"CuVwm6NYcxj7","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","import numpy as np\n","\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","model = Sequential()\n","#first layer input shape is 2 and the output is 8\n","model.add(Dense(8, input_dim=2))\n","# activation function of the first layer\n","model.add(Activation('tanh'))\n","# second layer (no more need to specify the size of the input, just the output), size 1\n","model.add(Dense(1))\n","# Activation function of the second layer\n","model.add(Activation('sigmoid'))\n","\n","sgd = SGD(lr=0.1)\n","model.compile(loss='binary_crossentropy', optimizer=sgd)\n","\n","model.fit(X, y, batch_size=1, epochs=1000)\n","print(model.predict_proba(X))\n","\n","# Question 1 : It's trained by the method of the stochastic Gradient Descent with a learning\n","# rate of 0.1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4sleZiOeqSMB","colab_type":"text"},"cell_type":"markdown","source":["Output layer has five nodes now"]},{"metadata":{"id":"93pgO0nqj56_","colab_type":"code","colab":{}},"cell_type":"code","source":["y_1 = np.array([[0],[1], [1],[0]]) #XOR gate\n","y_2 = np.array([[0],[0],[0], [1]]) # AND gate\n","y_3 = np.array([[0],[1],[1],[1]])  # OR gate\n","y_4 = np.array([[1],[1],[0],[0]])  # NOT x_1 gate\n","y_5 = np.array([[1],[0],[1],[0]])  # NOT x_2 gate\n","Y= np.concatenate([y_1, y_2, y_3, y_4, y_5], axis=1)\n","\n","model = Sequential()\n","#first layer : input size is 2 and the output is 8\n","model.add(Dense(8, input_dim=2))\n","# activation function of the first layer\n","model.add(Activation('tanh'))\n","# second layer (no more need to specify the size of the input, just the output), size 5\n","model.add(Dense(5))\n","# Activation function of the second layer\n","model.add(Activation('sigmoid'))\n","\n","sgd = SGD(lr=0.1)\n","model.compile(loss='binary_crossentropy', optimizer=sgd)\n","\n","model.fit(X, Y, batch_size=1, epochs=500)  # The more the parameter epochs is high the more the loss is low and vice-versa\n","print(model.predict_proba(X))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wVu6gE6Ve8tL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1201},"outputId":"75d35d2e-a8dc-480a-f012-1d9e8ead19b7","executionInfo":{"status":"ok","timestamp":1550059919477,"user_tz":-60,"elapsed":194125,"user":{"displayName":"Daouda Barry","photoUrl":"","userId":"11998833459857343407"}}},"cell_type":"code","source":["'''Trains a simple deep NN on the MNIST dataset.\n","Gets to 98.40% test accuracy after 20 epochs\n","(there is *a lot* of margin for parameter tuning).\n","2 seconds per epoch on a K520 GPU.\n","'''\n","\n","from __future__ import print_function\n","\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import RMSprop\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 20\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train = x_train.reshape(60000, 784)\n","x_test = x_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","model = Sequential()\n","model.add(Dense(512, activation='relu', input_shape=(784,)))\n","model.add(Dropout(0.2))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=RMSprop(),\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","60000 train samples\n","10000 test samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_27 (Dense)             (None, 512)               401920    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_29 (Dense)             (None, 10)                5130      \n","=================================================================\n","Total params: 669,706\n","Trainable params: 669,706\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/20\n","60000/60000 [==============================] - 10s 167us/step - loss: 0.2454 - acc: 0.9250 - val_loss: 0.1287 - val_acc: 0.9594\n","Epoch 2/20\n","60000/60000 [==============================] - 9s 157us/step - loss: 0.1022 - acc: 0.9694 - val_loss: 0.0776 - val_acc: 0.9776\n","Epoch 3/20\n","60000/60000 [==============================] - 10s 158us/step - loss: 0.0749 - acc: 0.9780 - val_loss: 0.0756 - val_acc: 0.9791\n","Epoch 4/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0595 - acc: 0.9818 - val_loss: 0.0801 - val_acc: 0.9795\n","Epoch 5/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0500 - acc: 0.9849 - val_loss: 0.0746 - val_acc: 0.9810\n","Epoch 6/20\n","60000/60000 [==============================] - 10s 160us/step - loss: 0.0433 - acc: 0.9870 - val_loss: 0.0914 - val_acc: 0.9768\n","Epoch 7/20\n","60000/60000 [==============================] - 10s 159us/step - loss: 0.0388 - acc: 0.9883 - val_loss: 0.0825 - val_acc: 0.9800\n","Epoch 8/20\n","60000/60000 [==============================] - 10s 160us/step - loss: 0.0374 - acc: 0.9895 - val_loss: 0.0793 - val_acc: 0.9837\n","Epoch 9/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0328 - acc: 0.9906 - val_loss: 0.0826 - val_acc: 0.9820\n","Epoch 10/20\n","60000/60000 [==============================] - 9s 157us/step - loss: 0.0284 - acc: 0.9915 - val_loss: 0.0761 - val_acc: 0.9852\n","Epoch 11/20\n","60000/60000 [==============================] - 10s 160us/step - loss: 0.0266 - acc: 0.9921 - val_loss: 0.0923 - val_acc: 0.9822\n","Epoch 12/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0252 - acc: 0.9925 - val_loss: 0.0970 - val_acc: 0.9838\n","Epoch 13/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0247 - acc: 0.9931 - val_loss: 0.0862 - val_acc: 0.9839\n","Epoch 14/20\n","60000/60000 [==============================] - 9s 158us/step - loss: 0.0234 - acc: 0.9937 - val_loss: 0.1029 - val_acc: 0.9829\n","Epoch 15/20\n","60000/60000 [==============================] - 10s 164us/step - loss: 0.0231 - acc: 0.9940 - val_loss: 0.0975 - val_acc: 0.9842\n","Epoch 16/20\n","60000/60000 [==============================] - 10s 160us/step - loss: 0.0228 - acc: 0.9944 - val_loss: 0.1015 - val_acc: 0.9815\n","Epoch 17/20\n","60000/60000 [==============================] - 9s 157us/step - loss: 0.0218 - acc: 0.9944 - val_loss: 0.1047 - val_acc: 0.9820\n","Epoch 18/20\n","60000/60000 [==============================] - 10s 160us/step - loss: 0.0212 - acc: 0.9943 - val_loss: 0.1101 - val_acc: 0.9808\n","Epoch 19/20\n","60000/60000 [==============================] - 10s 162us/step - loss: 0.0162 - acc: 0.9958 - val_loss: 0.1161 - val_acc: 0.9814\n","Epoch 20/20\n","60000/60000 [==============================] - 10s 161us/step - loss: 0.0175 - acc: 0.9955 - val_loss: 0.1205 - val_acc: 0.9840\n","Test loss: 0.12053492431526842\n","Test accuracy: 0.984\n"],"name":"stdout"}]},{"metadata":{"id":"h3mJ3bUWydan","colab_type":"text"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"bN9KgqYlvfQd","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]}]}